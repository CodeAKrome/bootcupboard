{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9cf7b0f-f6b8-41c4-9525-ef24bc4a5e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 1\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from llama_index import download_loader\n",
    "\n",
    "#PubmedReader = download_loader(\"PubmedReader\")\n",
    "PDFReader = download_loader(\"PDFReader\")\n",
    "#loader = PubmedReader()\n",
    "loader = PDFReader()\n",
    "#documents = loader.load_data(search_query=\"covid 19 vaccine\")\n",
    "documents = loader.load_data(file=Path(\"voyager.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a9c10b-6cd6-4f21-bce2-6098ee3fa5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='4be881ad-4d50-4a3e-ba07-a4a074f62b29', embedding=None, metadata={'page_label': '2', 'file_name': 'voyager.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='M i n e  W o o d   L o g\\nM a k e  C r a f t i n g  T a b l e\\nC r a f t  S t o n e  S w o r d\\nC r a f t  S h i e l d\\nM a k e  F u r n a c e\\nC o o k  S t e a k\\nC o m b a t  Z o m b i e     M i n e  W o o d  L o gM a k e  C r a f t i n g  T a b l eC o m b a t  \\nZ o m b i e\\nM i n e  D i a m o n d\\nN e w  \\nT a s kC o d e  a s  \\nA c t i o n sR e f i n e  P r o g r a mE n v  F e e d b a c k\\nE x e c u t i o n  E r r o r sU p d a t e  \\nE x p l o r a t i o n  \\nP r o g r e s sS k i l l  \\nR e t r i e v a l\\nA d d  N e w  S k i l lA u t o m a t i c  C u r r i c u l u mI t e r a t i v e  P r o m p t i n g  M e c h a n i s mS k i l l  L i b r a r y\\nE n v i r o n m e n tS e l f - V e r i f i c a t i o n\\nFigure 2: VOYAGER consists of three key components: an automatic curriculum for open-ended\\nexploration, a skill library for increasingly complex behaviors, and an iterative prompting mechanism\\nthat uses code as action space.\\n1 Introduction\\nBuilding generally capable embodied agents that continuously explore, plan, and develop new skills\\nin open-ended worlds is a grand challenge for the AI community [ 1–5]. Classical approaches\\nemploy reinforcement learning (RL) [ 6,7] and imitation learning [ 8–10] that operate on primitive\\nactions, which could be challenging for systematic exploration [ 11–15], interpretability [ 16–18], and\\ngeneralization [ 19–21]. Recent advances in large language model (LLM) based agents harness the\\nworld knowledge encapsulated in pre-trained LLMs to generate consistent action plans or executable\\npolicies [ 16,22,19]. They are applied to embodied tasks like games and robotics [ 23–27], as well as\\nNLP tasks without embodiment [ 28–30]. However, these agents are not lifelong learners that can\\nprogressively acquire, update, accumulate, and transfer knowledge over extended time spans [ 31,32].\\nLet us consider Minecraft as an example. Unlike most other games studied in AI [ 33,34,10],\\nMinecraft does not impose a predefined end goal or a fixed storyline but rather provides a unique\\nplayground with endless possibilities [ 23]. Minecraft requires players to explore vast, procedurally\\ngenerated 3D terrains and unlock a tech tree using gathered resources. Human players typically start\\nby learning the basics, such as mining wood and cooking food, before advancing to more complex\\ntasks like combating monsters and crafting diamond tools. We argue that an effective lifelong learning\\nagent should have similar capabilities as human players: (1) propose suitable tasks based on its\\ncurrent skill level and world state, e.g., learn to harvest sand and cactus before iron if it finds itself in\\na desert rather than a forest; (2) refine skills based on environmental feedback and commit mastered\\nskills to memory for future reuse in similar situations (e.g. fighting zombies is similar to fighting\\nspiders); (3) continually explore the world and seek out new tasks in a self-driven manner.\\nTowards these goals, we introduce VOYAGER , the first LLM-powered embodied lifelong learning\\nagent to drive exploration, master a wide range of skills, and make new discoveries continually\\nwithout human intervention in Minecraft. VOYAGER is made possible through three key modules\\n(Fig. 2): 1) an automatic curriculum that maximizes exploration; 2) a skill library for storing\\nand retrieving complex behaviors; and 3) a new iterative prompting mechanism that generates\\nexecutable code for embodied control. We opt to use code as the action space instead of low-level\\nmotor commands because programs can naturally represent temporally extended and compositional\\nactions [ 16,22], which are essential for many long-horizon tasks in Minecraft. VOYAGER interacts\\nwith a blackbox LLM (GPT-4 [ 35]) through prompting and in-context learning [ 36–38]. Our approach\\nbypasses the need for model parameter access and explicit gradient-based training or finetuning.\\nMore specifically, VOYAGER attempts to solve progressively harder tasks proposed by the automatic\\ncurriculum , which takes into account the exploration progress and the agent’s state. The curriculum\\nis generated by GPT-4 based on the overarching goal of “discovering as many diverse things as\\npossible”. This approach can be perceived as an in-context form of novelty search [39,40].VOYAGER\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e97a74-87cd-4ced-8d92-043a91325d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
